\section{H$_\infty$ Control}
% Abstract section


\subsection{Non data driven $\mathcal{H}_\infty$ control}



\Thr{\cite[Thr 4.6.6]{skelton1997unified}}{
	Let $\gamma > 0$. $A+BK$ is stable and $\| G(z) \|_{\mathcal{H}_\infty}$ if and only if there exists a matrix $P = P^\top > 0$ such that
	\begin{subequations}\label{HinfCondition}
		\begin{align} 
			P - (A + BK)^\top (P^{-1} - \frac{1}{\gamma^2}I)^{-1} (A + BK) - (C + DK)^\top (C + DK) &> 0 \\
			P^{-1} - \frac{1}{\gamma^2}I &> 0 
		\end{align}
	\end{subequations}
}


\subsection{Data driven $\mathcal{H}_\infty$ control}

We will once again extend the $\mathcal{H}_\infty$ control problem to data driven control. In our case we want to find a controller $K$ that stabilises all matrices $(A,B) \in \Sigma_{i/s/n}$ while still optimising $\gamma$ to be as small as possible. Using the above mentioned theorem we are able to come to the following definition for informativity for $\mathcal{H}_\infty$ control.
% What is Hinf

\Def{Informative for $\mathcal{H}_\infty$ control \cite[Def 18]{waarde2020noisy}}{
	The data $(U_-,X)$ is informative for $\mathcal{H}_\infty$ control with performance $\gamma$ if there exists matrices $P = P^\top > 0$ and $K$ such that:
	\begin{align} \tag{\ref{HinfCondition}a}
		P - (A + BK)^\top (P^{-1} - \frac{1}{\gamma^2}I)^{-1}(A + BK) - (C+DK)^\top (C+DK) &> 0 \\
		P^{-1} - \frac{1}{\gamma^2}I &> 0 \tag{\ref{HinfCondition}b}
	\end{align}
	Holds for all $(A,B) \in \Sigma_{i/s/n}$.
}

Similar to the quadratic stabilisation and $\mathcal{H}_2$ stabilisation, the only thing left is to rewrite (\ref{HinfCondition}a) to be of the form:
\begin{equation*}
	\begin{bmatrix} I \\ A^\top \\ B^\top \end{bmatrix}^\top
	M
	\begin{bmatrix} I \\ A^\top \\ B^\top \end{bmatrix} > 0
\end{equation*}

To achieve this we will multiply (\ref{HinfCondition}a) with $P^{-1}$ from both sides. We will define the following variables, $Y = P^{-1}$ and $L = KY$.
\begin{align*}
	Y - (AY + BL)^\top (Y - \frac{1}{\gamma^2} I )^{-1} (AY + BL) - (CY + DL)^\top (CY + DL) &> 0
\end{align*}

Using this we can rewrite the first inequality in the following form.

\begin{equation*}
	\left[\begin{array}{c}
		I \\ \hline A^\top \\ B^\top
	\end{array}\right]^\top
	\left[\begin{array}{c|c}
		Y - (CY + DL)^\top (CY + DL) & 0 \\ \hline
		0 & -\begin{bmatrix} Y\\ L \end{bmatrix} \left(Y - \frac{1}{\gamma^2}I\right)^{-1} \begin{bmatrix} Y \\ L \end{bmatrix}^\top
	\end{array}\right]
	\left[\begin{array}{c}
		I \\ \hline A^\top \\ B^\top
	\end{array}\right] > 0
\end{equation*}

% State theorem
\Thr{\cite[Thr 13]{waarde2020noisy}}{
	Assume that the generalised Slater condition (\ref{GenerelisedSlaterCondition}) holds for $N$ in (\ref{NData}) and some $\bar{Z} \in \mathbb{R}^{(n+m_ \times n)}$. Then the data $(U_-,X)$ is informative for $\mathcal{H}_\infty$ control with performance $\gamma$ if and only if there exists matrices $Y = Y^\top > 0$ and $L$, and scalars $\alpha \geq 0$ and $\beta > 0$ satisfying:
	\begin{equation*}
		\begin{bmatrix}
			Y - \beta I & 0 & 0 & 0 & (CY + DL)^\top \\
			0 & 0 & 0 & Y & 0 \\
			0 & 0 & 0 & L & 0 \\
			0 & Y & L^\top & Y - \frac{1}{\gamma^2}I & 0 \\
			CY + DL & 0 & 0 & 0 & I
		\end{bmatrix} - \alpha
		\begin{bmatrix} I&X_+ \\ 0 & -X_- \\ 0&-U_- \\ 0&0 \\ 0&0 \end{bmatrix}
		\begin{bmatrix} \Phi_{11} & \Phi_{12} \\ \Phi_{12}^\top & \Phi_{22} \end{bmatrix}
		\begin{bmatrix} I&X_+ \\ 0 & -X_- \\ 0&-U_- \\ 0&0 \\ 0&0 \end{bmatrix}^\top \geq 0
	\end{equation*}
	\begin{equation*}
		Y - \frac{1}{\gamma^2}I > 0
	\end{equation*}
	Moreover, if $Y$ and $L$ satisfy the above inequalities, then $K = LY^{-1}$ is such that $A + BK$ is stable and $\| G(z)\|_{\mathcal{H}_\infty} < \gamma$ for all $(A,B) \in \Sigma$. \todo{Explain G(z)}
}

% Pseudo code / algorithm
\subsection{Implementation}
\subsubsection*{Syntax} 
\mon{[bool, K, diagnostics, gamma, info] = isInformHInf(X, U, Phi, C, D)} \\
\mon{[bool, K, diagnostics, gamma, info] = isInformHInf(X, U, Phi, C, D, tolerance)} \\
\mon{[bool, K, diagnostics, gamma, info] = isInformHInf(X, U, Phi, C, D, tolerance, options)} 

\subsubsection*{Description}\todo{a}
\mon{[[bool, K, diagnostics, gamma, info] = isInformHInf(X, U, Phi, C, D)}: Checks is the data is informative for \mbox{H$\infty$} control. \\
\mon{[bool, K, diagnostics, gamma, info] = isInformHInf(X, U, Phi, C, D, tolerance)}: Checks is the data is informative for \mbox{H$\infty$} control using a given tolerance. \\
\mon{[bool, K, diagnostics, gamma, info] = isInformHInf(X, U, Phi, C, D, tolerance, options)}: Checks is the data is informative for \mbox{H$\infty$} control using a given tolerance and a given spdsetting object from Yalmip.

\subsubsection*{Input arguments}
\textbf{\mon{X}}: State data matrix of dimension $n \times T+1$ from a input/state data set.\\
\textbf{\mon{U}}: Input data matrix of dimension $m \times T$ from a input/state data set.\\
\textbf{\mon{Phi}}: Noise matrix as in (\ref{noiseBound}). \\ 
\textbf{\mon{C}}: State performance matrix. \\ 
\textbf{\mon{D}}: State performance matrix. \\ 
\textbf{\mon{tolerance}}: Tolerance used for determining when a value is zero up to machine precision. Default value is \mon{1e-14}.\\
\textbf{\mon{options}}: sdpsettings used with the Yalmip solver.

\subsubsection*{Output arguments}
\textbf{\mon{bool}}: (boolean) True if the data is informative for quadratic stabilisation, false otherwise. If false then the \mon{info} variable can be check to find which condition failed. \\
\textbf{\mon{K}}: (matrix) If the data is informative, it contains a stabilising controller \mon{K} for closed loop control \mon{A+BK}, empty otherwise.\\
\textbf{\mon{diagnostics}}: (struct) Diagnostics from the Yalmip \mon{optimize()} function. \\
\textbf{\mon{gamma}}: (double) Value of gamma as found by the solver. \\
\textbf{\mon{info}}: (int) Diagnostic variable use to identify which conditions (if any) failed. The verification is done on the solution obtained from Yalmip. For information about the type of error use the \mon{help} command in Matlab


% Example using function
\subsection{Example}
In this example we will consider data generated by the following system.
\begin{align*}
	x(t+1) &= -1\frac{1}{2}x(t) + u(t) \\
	z(t)   &=  1\frac{1}{2}x(t) + \frac{1}{2}u(t)
\end{align*}
We will generate our data using the following random input and noise sequences. The input sequence is uniformly sample on the interval $[-1,1]$ and the noise is uniformly sampled from the interval $[-0.1, 0.1]$.
\begin{align*}
	W_- &= \begin{bmatrix}  0.0629 &   0.0812 &  -0.0746 &   0.0827 &   0.0265 \end{bmatrix} \\
	U_- &= \begin{bmatrix} -0.8049 &  -0.4430 &   0.0938 &   0.9150 &   0.9298 \end{bmatrix} \\
	X   &= \begin{bmatrix}  1.0000 &  -2.2420 &   3.0012 &  -4.4826 &   7.7216 & -10.6261 \end{bmatrix}
\end{align*}

We can use the same $\Phi$ matrix that we used in examples (\ref{ExampleQS}) and (\ref{ExampleH2}) since we have the same number of data points and the noise was sampled from the same distribution. 

\begin{equation*}
\Phi = \begin{bmatrix} 0.05 & \overline{0} \\ \overline{0}^\top & -I_5 \end{bmatrix}
\end{equation*}

Before we attempt to find a solution, we will first need to check the Slater condition. For this we need that the following matrix has at least 1 positive eigenvalue.

\begin{equation*}
N = 
\begin{bmatrix} I&X_+ \\ 0 & -X_- \\ 0&-U_- \end{bmatrix}
\begin{bmatrix} 0.05 & \overline{0} \\ \overline{0}^\top & -I_5 \end{bmatrix}
\begin{bmatrix} I&X_+ \\ 0 & -X_- \\ 0&-U_- \end{bmatrix}^\top 
=
\begin{bmatrix} 
 -206.6146 &-139.0872 &  -2.7603\\
 -139.0872 & -94.7506 &  -3.5478\\
   -2.7603 &  -3.5478 &  -2.5547
\end{bmatrix} 
\end{equation*}

\begin{equation*}
\sigma(N) = \{ \begin{array}{ccc}
	-300.6559 & -3.2707 & 0.0068
\end{array} \}
\end{equation*}

Hence the Slater condition is satisfied. Now we need to find a symmetric positive definite matrix $Y$, a matrix $L$, a non-negative scaler $\alpha$ and a positive scalar $\beta$ such that the following holds.

\begin{equation*}
	\begin{bmatrix}
		Y - \beta I & 0 & 0 & 0 & (CY + DL)^\top \\
		0 & 0 & 0 & Y & 0 \\
		0 & 0 & 0 & L & 0 \\
		0 & Y & L^\top & Y - \frac{1}{\gamma^2}I & 0 \\
	CY + DL & 0 & 0 & 0 & I
	\end{bmatrix} - \alpha
	\begin{bmatrix} I&X_+ \\ 0 & -X_- \\ 0&-U_- \\ 0&0 \\ 0&0 \end{bmatrix}
	\begin{bmatrix} \Phi_{11} & \Phi_{12} \\ \Phi_{12}^\top & \Phi_{22} \end{bmatrix}
	\begin{bmatrix} I&X_+ \\ 0 & -X_- \\ 0&-U_- \\ 0&0 \\ 0&0 \end{bmatrix}^\top \geq 0
\end{equation*}
\begin{equation*}
	Y - \frac{1}{\gamma^2}I > 0
\end{equation*}

Using numerical tools such as Yalmip we are able to find the following values such that the equations are satisfied.
\begin{align*}
	Y      &= 0.16\\
	L      &= 0.25\\
	\alpha &= 0.9\\
	\beta  &= 1e-06
\end{align*}

Using these values we are able to retrieve the feedback gain $K = 1.5625$. If we look at the closed loop system we can see that it is indeed stabilised.
\begin{equation*}
	A + BK = 6.25e-2
\end{equation*}

We will use (\ref{HinfCondition}) to compute $\gamma$.


\begin{lstlisting}
gamma2inv = sdpvar(1);
invYGamma = sdpvar(1);

Cond = [ Y - (A * Y + B * L)' * invYGamma * (A * Y + B * L) - (C * Y + D * L)' * (C * Y + D * L) >= 1e-8 ];
Cond = Cond + [ Y - gamma2inv >= 1e-8 ];
Cond = Cond + [ gamma2inv >= 1e-8 ];
Cond = Cond + [ (Y - gamma2inv) * invYGamma == eye(1) ];
\end{lstlisting}


\todo{Finish example}
























